# Speech Emotion Recognition

This repository contains the implementation of a Speech Emotion Recognition system, developed as part of a Machine Learning internship. The project aims to automatically detect emotions from speech signals using machine learning and signal processing techniques.

## Project Overview

The Speech Emotion Recognition system processes audio data to classify emotions such as anger, happiness, sadness, and more. The project involves the following steps:

1. Data Collection and Preprocessing
   - Collected the Toronto Emotional Speech Set (TESS) dataset.
   - Cleaned and standardized the audio data for consistency.

2. Feature Extraction
   - Extracted Mel Frequency Cepstral Coefficients (MFCCs) from audio files to capture spectral characteristics.

3. Model Development
   - Built and trained an LSTM model to classify emotions based on the extracted features.

4. Evaluation and Deployment
   - Evaluated the model's performance using accuracy and F1-score metrics.
   - Deployed the model to recognize emotions in real-time applications.

## Dataset

The dataset used in this project is the Toronto Emotional Speech Set (TESS), which includes speech samples labeled with corresponding emotions.
